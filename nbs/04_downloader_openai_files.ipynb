{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp scripts/downloader_openai_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_320567/1838813386.py:7: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import argparse\n",
    "from io import BytesIO\n",
    "import sys\n",
    "from typing import List\n",
    "import openai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _prepare_splits(df_content: pd.DataFrame, df_splits: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_content.merge(df_splits, left_on=\"url\", right_on=\"url\")\n",
    "    records = []\n",
    "    for split, sub_df in df.groupby(\"split\"):\n",
    "        records.append({\n",
    "            \"split\": split,\n",
    "            \"content\": \"\\n\\n\".join(sub_df[\"content\"])\n",
    "        })\n",
    "    return pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>None\\n# Потери России в войне с Украиной. Свод...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split                                            content\n",
       "0      0  None\\n# Потери России в войне с Украиной. Свод..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_prepare_splits(\n",
    "    pd.read_json(\"../data/urls--downloaded-markdown.jsonl\", orient=\"records\", lines=True),\n",
    "    pd.read_json(\"../data/urls--downloaded-markdown--splits.jsonl\", orient=\"records\", lines=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _upload_splits(df: pd.DataFrame, openai_client: openai.OpenAI) -> pd.DataFrame:\n",
    "    file_ids = []\n",
    "    for split, content in zip(df[\"split\"], df[\"content\"]):\n",
    "        file = BytesIO(content.encode(\"utf-8\"))\n",
    "        file.name = f\"split-{split}.md\"\n",
    "        file.seek(0)\n",
    "        openai_file = openai_client.files.create(\n",
    "            file=file,\n",
    "            purpose=\"assistants\"\n",
    "        )\n",
    "        file_ids.append(openai_file.id)\n",
    "    return pd.DataFrame({\"file_id\": file_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file-akaDtoQagPRsscgUFmz53vqM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         file_id\n",
       "0  file-akaDtoQagPRsscgUFmz53vqM"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_upload_splits(\n",
    "    _prepare_splits(\n",
    "        pd.read_json(\"../data/urls--downloaded-markdown.jsonl\", orient=\"records\", lines=True),\n",
    "        pd.read_json(\"../data/urls--downloaded-markdown--splits.jsonl\", orient=\"records\", lines=True),\n",
    "    ),\n",
    "    openai_client=openai.OpenAI(\n",
    "        api_key=open(\"../data/openaiapikey.txt\").read()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process(file_name_content: str, file_name_splits: str, file_name_openai_api_key: str, file_name_file_ids: str) -> None:\n",
    "    df = _prepare_splits(\n",
    "        pd.read_json(file_name_content, orient=\"records\", lines=True),\n",
    "        pd.read_json(file_name_splits, orient=\"records\", lines=True),\n",
    "    )\n",
    "    with open(file_name_openai_api_key, \"r\") as src:\n",
    "        openai_api_key = src.read()\n",
    "    df_response = _upload_splits(df, openai.OpenAI(api_key=openai_api_key))\n",
    "    df_response.to_json(file_name_file_ids, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "if __name__ == \"__main__\" and \"ipykernel_launcher\" not in \" \".join(sys.argv):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--file_name_content\",\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"JSONL file with downloaded Markdown\")\n",
    "    parser.add_argument(\"--file_name_splits\",\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"JSONL file with precalculated splits\")\n",
    "    parser.add_argument(\"--file_name_openai_api_key\",\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"File with OpenAI api key\")\n",
    "    parser.add_argument(\"--file_name_file_ids\",\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"JSONL file with OpenAI file ids\")\n",
    "    process(**vars(parser.parse_args()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
