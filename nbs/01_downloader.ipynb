{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp scripts/downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_314608/912167555.py:9: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import argparse\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "import sys\n",
    "from typing import Union, List\n",
    "from bs4 import BeautifulSoup\n",
    "from goose3 import Goose\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PageData(BaseModel):\n",
    "    url: str\n",
    "    title: str\n",
    "    content: str\n",
    "    publish_date: str\n",
    "\n",
    "\n",
    "def _load_page_html(url: str) -> str:\n",
    "    response = requests.get(url)\n",
    "    assert response.status_code == 200\n",
    "    return response.content\n",
    "\n",
    "\n",
    "def _remove_scripts_and_styles(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    for script in soup.find_all('script'):\n",
    "        script.decompose()\n",
    "    for style in soup.find_all('style'):\n",
    "        style.decompose()\n",
    "    return str(soup)\n",
    "\n",
    "\n",
    "def _parse_page(url: str, goose: Goose) -> Union[PageData, None]:\n",
    "    try:\n",
    "        html = _load_page_html(url)\n",
    "        html = _remove_scripts_and_styles(html)\n",
    "        extraction = goose.extract(raw_html=html)\n",
    "        return PageData(\n",
    "            url=url,\n",
    "            title=extraction.title,\n",
    "            content=extraction.top_node_raw_html,\n",
    "            publish_date=str(extraction.publish_date),\n",
    "        )\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def _parse_pages(urls: List[str], goose: Goose, thread_pool_size: int) -> List[PageData]:\n",
    "    pool = ThreadPoolExecutor(max_workers=thread_pool_size)\n",
    "    results = list(pool.map(\n",
    "        _parse_page,\n",
    "        urls,\n",
    "        [goose] * len(urls)\n",
    "    ))\n",
    "    results = [item for item in results if item is not None]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def process(file_name_urls: str, file_name_content: str, thread_pool_size: int) -> None:\n",
    "    print(\"Reading URLs\")\n",
    "    with open(file_name_urls, \"r\", encoding=\"utf-8\") as src:\n",
    "        urls = src.read().strip().split(\"\\n\")\n",
    "    print(\"Reading downloaded content\")\n",
    "    if os.path.exists(file_name_content):\n",
    "        df_content = pd.read_json(file_name_content, orient=\"records\", lines=True)\n",
    "    else:\n",
    "        df_content = pd.DataFrame({\"url\": [], \"title\": [], \"content\": [], \"publish_date\": []})\n",
    "    print(\"Removing unnecessary content\")\n",
    "    df_content = df_content.loc[df_content[\"url\"].isin(urls)]\n",
    "    print(\"Downloading new content\")\n",
    "    new_content = _parse_pages(\n",
    "        [url for url in urls if url not in set(df_content[\"url\"])],\n",
    "        Goose(),\n",
    "        thread_pool_size,\n",
    "    )\n",
    "    df_new_content = pd.DataFrame.from_records([\n",
    "        item.model_dump()\n",
    "        for item in new_content\n",
    "    ])\n",
    "    print(\"Saving new content\")\n",
    "    df_content = pd.concat([df_content, df_new_content]).reset_index(drop=True)\n",
    "    df_content.to_json(file_name_content, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading URLs\n",
      "Reading downloaded content\n",
      "Removing unnecessary content\n",
      "Downloading new content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Publish date 05:20, 22 февраля 2024 could not be resolved to UTC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving new content\n"
     ]
    }
   ],
   "source": [
    "process(\n",
    "    \"../data/urls.txt\",\n",
    "    \"../data/urls--downloaded.jsonl\",\n",
    "    8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "if __name__ == \"__main__\" and \"ipykernel_launcher\" not in \" \".join(sys.argv):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--file_name_urls\",\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"List with content URLs\")\n",
    "    parser.add_argument(\"--file_name_content\",\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"List with downloaded HTML\")\n",
    "    parser.add_argument(\"--thread_pool_size\",\n",
    "                        type=int,\n",
    "                        required=True,\n",
    "                        help=\"Downloader thread pool size\")\n",
    "    parser.parse_args(**vars(parser.parse_args()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
