{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp scripts/downloader_files_split_calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_320147/350994993.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import argparse\n",
    "import sys\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "MAX_FILE_SIZE = 512 * 1024 * 1024\n",
    "MAX_FILE_TOKENS = 2000000\n",
    "TOKENIZER_MODEL = \"gpt-4-turbo-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _calculate_splits(df: pd.DataFrame, encoding: tiktoken.Encoding) -> List[int]:\n",
    "    df[\"size_bytes\"] = (df[\"content\"] + \"\\n\\n\").apply(lambda text: len(text.encode(\"utf-8\")))\n",
    "    df[\"size_tokens\"] = (df[\"content\"] + \"\\n\\n\").apply(lambda text: len(encoding.encode(text)))\n",
    "    cum_size_bytes = 0\n",
    "    cum_size_tokens = 0\n",
    "    file_idx = 0\n",
    "    files = []\n",
    "    for _, row in df.iterrows():\n",
    "        cum_size_bytes += row[\"size_bytes\"]\n",
    "        cum_size_tokens += row[\"size_tokens\"]\n",
    "        if cum_size_bytes > MAX_FILE_SIZE or cum_size_tokens > MAX_FILE_TOKENS:\n",
    "            file_idx += 1\n",
    "            cum_size_bytes = row[\"size_bytes\"]\n",
    "            cum_size_tokens = row[\"size_tokens\"]\n",
    "        files.append(file_idx)\n",
    "    return pd.Series(files, index=df.index)\n",
    "\n",
    "\n",
    "def process(file_name_content: str, file_name_splits: str) -> None:\n",
    "    df = pd.read_json(file_name_content, orient=\"records\", lines=True)\n",
    "    encoding = tiktoken.encoding_for_model(TOKENIZER_MODEL)\n",
    "    df[\"split\"] = _calculate_splits(df, encoding)\n",
    "    df[[\"url\", \"split\"]].to_json(file_name_splits, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process(\n",
    "    \"../data/urls--downloaded-markdown.jsonl\",\n",
    "    \"../data/urls--downloaded-markdown--splits.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "if __name__ == \"__main__\" and \"ipykernel_launcher\" not in \" \".join(sys.argv):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--file_name_content\",\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"JSONL file with downloaded Markdown\")\n",
    "    parser.add_argument(\"--file_name_splits\",\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"JSONL file with calculated splits\")\n",
    "    process(**vars(parser.parse_args()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
