# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_downloader_openai_files_split_calculate.ipynb.

# %% auto 0
__all__ = ['MAX_FILE_SIZE', 'MAX_FILE_TOKENS', 'TOKENIZER_MODEL', 'process']

# %% ../nbs/03_downloader_openai_files_split_calculate.ipynb 1
import argparse
import sys
from typing import List
import pandas as pd
import tiktoken

# %% ../nbs/03_downloader_openai_files_split_calculate.ipynb 2
MAX_FILE_SIZE = 512 * 1024 * 1024
MAX_FILE_TOKENS = 2000000
TOKENIZER_MODEL = "gpt-4-turbo-preview"

# %% ../nbs/03_downloader_openai_files_split_calculate.ipynb 3
def _calculate_splits(df: pd.DataFrame, encoding: tiktoken.Encoding) -> List[int]:
    df["size_bytes"] = (df["content"] + "\n\n").apply(lambda text: len(text.encode("utf-8")))
    df["size_tokens"] = (df["content"] + "\n\n").apply(lambda text: len(encoding.encode(text)))
    cum_size_bytes = 0
    cum_size_tokens = 0
    file_idx = 0
    files = []
    for _, row in df.iterrows():
        cum_size_bytes += row["size_bytes"]
        cum_size_tokens += row["size_tokens"]
        if cum_size_bytes > MAX_FILE_SIZE or cum_size_tokens > MAX_FILE_TOKENS:
            file_idx += 1
            cum_size_bytes = row["size_bytes"]
            cum_size_tokens = row["size_tokens"]
        files.append(file_idx)
    return pd.Series(files, index=df.index)


def process(file_name_content: str, file_name_splits: str) -> None:
    df = pd.read_json(file_name_content, orient="records", lines=True)
    encoding = tiktoken.encoding_for_model(TOKENIZER_MODEL)
    df["split"] = _calculate_splits(df, encoding)
    df[["url", "split"]].to_json(file_name_splits, orient="records", lines=True)

# %% ../nbs/03_downloader_openai_files_split_calculate.ipynb 5
if __name__ == "__main__" and "ipykernel_launcher" not in " ".join(sys.argv):
    parser = argparse.ArgumentParser()
    parser.add_argument("--file_name_content",
                        type=str,
                        required=True,
                        help="JSONL file with downloaded Markdown")
    parser.add_argument("--file_name_splits",
                        type=str,
                        required=True,
                        help="JSONL file with calculated splits")
    process(**vars(parser.parse_args()))
